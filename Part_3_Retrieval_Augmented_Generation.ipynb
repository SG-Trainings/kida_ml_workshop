{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IE3-yKCU-MWj"
      },
      "source": [
        "# Building a RAG with Hugging Face and Milvus\n",
        "\n",
        "The RAG system combines a retrieval system with an LLM. The system first retrieves relevant documents from a corpus using Milvus vector database, then uses an LLM hosted in Hugging Face to generate answers based on the retrieved documents."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "J9Gt9_lX-MWk",
        "jupyter": {
          "outputs_hidden": false
        }
      },
      "source": [
        "## Preparation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "vfeSxiJq-MWl",
        "jupyter": {
          "outputs_hidden": false
        }
      },
      "source": [
        "We recommend that you configure your [Hugging Face User Access Token](https://huggingface.co/docs/hub/security-tokens), and set it in your environment variables because we will use a LLM from the Hugging Face Hub. You may get a low limit of requests if you don't set the token environment variable."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "FyTaX9wY-MWl",
        "jupyter": {
          "outputs_hidden": false
        },
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "os.environ[\"HF_TOKEN\"] = \"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xOM9mWsB-MWm"
      },
      "source": [
        "### Prepare the data\n",
        "\n",
        "We use the [AI Act PDF](https://artificialintelligenceact.eu/wp-content/uploads/2021/08/The-AI-Act.pdf), a regulatory framework for AI with different risk levels corresponding to more or less regulation, as the private knowledge in our RAG."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "sHJZbREh-MWm",
        "vscode": {
          "languageId": "shellscript"
        }
      },
      "outputs": [],
      "source": [
        "%%bash\n",
        "\n",
        "if [ ! -f \"The-AI-Act.pdf\" ]; then\n",
        "    wget -q https://artificialintelligenceact.eu/wp-content/uploads/2021/08/The-AI-Act.pdf\n",
        "fi"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "07bQkhHt-MWm"
      },
      "source": [
        "We use the [`PyPDFLoader`](https://python.langchain.com/v0.1/docs/modules/data_connection/document_loaders/pdf/) from LangChain to extract the text from the PDF, and then split the text into smaller chunks. By default, we set the chunk size as 1000 and the overlap as 200, which means each chunk will nearly have 1000 characters and the overlap between two chunks will be 200 characters."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uceyLBi0-MWn",
        "outputId": "0020a854-eb15-4d33-94a2-03c651ba4da0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "108\n"
          ]
        }
      ],
      "source": [
        "from langchain_community.document_loaders import PyPDFLoader\n",
        "\n",
        "loader = PyPDFLoader(\"The-AI-Act.pdf\")\n",
        "docs = loader.load()\n",
        "print(len(docs))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "XM1dqVrM-MWn"
      },
      "outputs": [],
      "source": [
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "\n",
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
        "chunks = text_splitter.split_documents(docs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "npsODSlf-MWn"
      },
      "outputs": [],
      "source": [
        "text_lines = [chunk.page_content for chunk in chunks]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2ZUKB0nZ-MWn"
      },
      "source": [
        "### Prepare the Embedding Model\n",
        "Define a function to generate text embeddings. We use [BGE embedding model](https://huggingface.co/BAAI/bge-small-en-v1.5) as an example, but you can use any embedding models, such as those found on the [MTEB leaderboard](https://huggingface.co/spaces/mteb/leaderboard)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "gT12QEdX-MWn"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "9b69d2000bab452caa6287a0e79c7abd",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "032ae45b37264400b1cadb68de7d9bcf",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config_sentence_transformers.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "70ea0dd2ad9643f08a4c48bcbe43bc37",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "README.md:   0%|          | 0.00/94.8k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "49c79ef9f1cf46c49859d94eb9b5f90c",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "sentence_bert_config.json:   0%|          | 0.00/52.0 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "2fc2960f4f304e72af575eb808a460ad",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config.json:   0%|          | 0.00/743 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b04111979a244f8b86d0a4bfe6791b96",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/133M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "dc579df5c0ee49fcb9e29de8111d3af7",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/366 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "89060f5ce5b74f0691b1657f9f16a591",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e333bbae778c4c88974124258c9aae5b",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/711k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "53ebebeedcfe4986a0900298efbf971a",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/125 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "461f097eeaf640cc97a2f5d0e844fd6a",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "1_Pooling%2Fconfig.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "embedding_model = SentenceTransformer(\"BAAI/bge-small-en-v1.5\")\n",
        "\n",
        "def emb_text(text):\n",
        "    return embedding_model.encode([text], normalize_embeddings=True).tolist()[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "__GRdtFS-MWn"
      },
      "source": [
        "Generate a test embedding and print its dimension and first few elements."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FyxTuIZ--MWo",
        "outputId": "05f1f0b1-8997-4fb1-cf54-41c1da7a722f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "384\n",
            "[-0.021639352664351463, -0.04694868251681328, 0.007031530141830444, -0.017096107825636864, -0.020527230575680733, -0.02209441177546978, 0.04893404617905617, 0.012171009555459023, 0.01683473400771618, 0.03607108071446419]\n"
          ]
        }
      ],
      "source": [
        "test_embedding = emb_text(\"This is a fish\")\n",
        "embedding_dim = len(test_embedding)\n",
        "print(embedding_dim)\n",
        "print(test_embedding[:10])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eaRPpSIc-MWo"
      },
      "source": [
        "## Load data into Milvus"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r3SgDVfS-MWo"
      },
      "source": [
        "### Create the Collection"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "ilI1qULX-MWo"
      },
      "outputs": [],
      "source": [
        "from pymilvus import MilvusClient\n",
        "\n",
        "milvus_client = MilvusClient(uri=\"./hf_milvus_demo.db\")\n",
        "\n",
        "collection_name = \"rag_collection\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "r3rqXo5Y-MWo",
        "jupyter": {
          "outputs_hidden": false
        }
      },
      "source": [
        "> As for the argument of `MilvusClient`:\n",
        "> - Setting the `uri` as a local file, e.g.`./hf_milvus_demo.db`, is the most convenient method, as it automatically utilizes [Milvus Lite](https://milvus.io/docs/milvus_lite.md) to store all data in this file.\n",
        "> - If you have a large amount of data, say more than a million vectors, you would need to set up a more performant Milvus server."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1v3WsIOt-MWo"
      },
      "source": [
        "Check if the collection already exists and drop it if it does."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "k7NcpAZI-MWo"
      },
      "outputs": [],
      "source": [
        "if milvus_client.has_collection(collection_name):\n",
        "    milvus_client.drop_collection(collection_name)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CoMUF220-MWo"
      },
      "source": [
        "Create a new collection with specified parameters.\n",
        "\n",
        "If we don't specify any field information, Milvus will automatically create a default `id` field for primary key, and a `vector` field to store the vector data. A reserved JSON field is used to store non-schema-defined fields and their values."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "Ko0inmYi-MWo"
      },
      "outputs": [],
      "source": [
        "milvus_client.create_collection(\n",
        "    collection_name=collection_name,\n",
        "    dimension=embedding_dim,\n",
        "    metric_type=\"IP\",  # Inner product distance\n",
        "    consistency_level=\"Strong\",  # Strong consistency level\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KyHAeGco-MWo"
      },
      "source": [
        "### Insert data\n",
        "Iterate through the text lines, create embeddings, and then insert the data into Milvus.\n",
        "\n",
        "Here is a new field `text`, which is a non-defined field in the collection schema. It will be automatically added to the reserved JSON dynamic field, which can be treated as a normal field at a high level."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ID9csqS0-MWo",
        "outputId": "e2761e0f-4674-4add-9638-74070a0e7ddc"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Creating embeddings: 100%|██████████| 424/424 [00:13<00:00, 31.20it/s]\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "424"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "data = []\n",
        "\n",
        "for i, line in enumerate(tqdm(text_lines, desc=\"Creating embeddings\")):\n",
        "    data.append({\"id\": i, \"vector\": emb_text(line), \"text\": line})\n",
        "\n",
        "insert_res = milvus_client.insert(collection_name=collection_name, data=data)\n",
        "insert_res[\"insert_count\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "puU1RWQo-MWo"
      },
      "source": [
        "## Build RAG\n",
        "\n",
        "### Retrieve data for a query\n",
        "\n",
        "Let's specify a question to ask about the corpus."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "miLF3XqE-MWo"
      },
      "outputs": [],
      "source": [
        "question = \"What are forbidden AI applications?\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lRigE0lz-MWo"
      },
      "source": [
        "Search for the question in the collection and retrieve the top 3 semantic matches."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "k7Ai9frE-MWp"
      },
      "outputs": [],
      "source": [
        "search_res = milvus_client.search(\n",
        "    collection_name=collection_name,\n",
        "    data=[\n",
        "        emb_text(question)\n",
        "    ],  # Use the `emb_text` function to convert the question to an embedding vector\n",
        "    limit=3,  # Return top 3 results\n",
        "    search_params={\"metric_type\": \"IP\", \"params\": {}},  # Inner product distance\n",
        "    output_fields=[\"text\"],  # Return the text field\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gLmIDMiQ-MWp"
      },
      "source": [
        "Let's take a look at the search results of the query\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J9TuxuSp-MWp",
        "outputId": "37457ef8-e6e0-4875-d79e-a79bdd2d6991"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[\n",
            "    [\n",
            "        \"techniques and approaches listed therein.  \\nTITLE II \\nPROHIBITED ARTIFICIAL INTELLIGENCE PRACTICES \\nArticle 5 \\n1. The following artificial intelligence practices shall be prohibited: \\n(a) the placing on the market, putting into service or use of an A I system that \\ndeploys subliminal techniques beyond a person\\u2019s consciousness in order to \\nmaterially distort a person\\u2019s behaviour in a manner that causes or is likely to \\ncause that person or another person physical or psychological harm; \\n(b) the placing on t he market, putting into service or use of an AI system that \\nexploits any of the vulnerabilities of a specific group of persons due to their \\nage, physical or mental disability, in order to materially distort the behaviour of \\na person pertaining to that grou p in a manner that causes or is likely to cause \\nthat person or another person physical or psychological harm; \\n(c) the placing on the market, putting into service or use of AI systems by public\",\n",
            "        0.7511818408966064\n",
            "    ],\n",
            "    [\n",
            "        \"Title II  establishes a list of prohibited AI. The regulation follows a risk -based approach,  \\ndifferentiating between uses of AI that create (i) an unacceptable risk, (ii) a high risk, and (iii) \\nlow or minimal risk. The list of prohibited practices in Title II comprises all those AI systems \\nwhose use is considered unacceptable as contravening Unio n values, for instance by violating \\nfundamental rights. The prohibitions covers practices that have a significant potential to \\nmanipulate persons  through subliminal techniques beyond their consciousness or exploit\",\n",
            "        0.7481958866119385\n",
            "    ],\n",
            "    [\n",
            "        \"EN 13  EN \\nvulnerabilities of specific vulnerable gro ups such as children or persons with disabilities in \\norder to materially distort their behaviour in a manner that is likely to cause them or another \\nperson psychological or physical harm. Other manipulative or exploitative practices affecting \\nadults that m ight be facilitated by AI systems could be covered by the existing data \\nprotection, consumer protection and digital service legislation that guarantee that natural \\npersons are properly informed and have free choice not to be subject to profiling or other \\npractices that might affect their behaviour. The proposal also prohibits AI -based social \\nscoring for general purposes done by public authorities. Finally, the use of \\u2018real time\\u2019 remote \\nbiometric identification systems in publicly accessible spaces for the purpose of law \\nenforcement is also prohibited unless certain limited exceptions apply. \\n5.2.3. HIGH-RISK AI SYSTEMS (TITLE III)\",\n",
            "        0.7364389896392822\n",
            "    ]\n",
            "]\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "\n",
        "retrieved_lines_with_distances = [\n",
        "    (res[\"entity\"][\"text\"], res[\"distance\"]) for res in search_res[0]\n",
        "]\n",
        "print(json.dumps(retrieved_lines_with_distances, indent=4))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1s5b26e3-MWp"
      },
      "source": [
        "### Use LLM to get an RAG response\n",
        "\n",
        "Before composing the prompt for LLM, let's first flatten the retrieved document list into a plain string."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "69SyVecE-MWp"
      },
      "outputs": [],
      "source": [
        "context = \"\\n\".join(\n",
        "    [line_with_distance[0] for line_with_distance in retrieved_lines_with_distances]\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JdOWKS0F-MWp"
      },
      "source": [
        "Define prompts for the Language Model. This prompt is assembled with the retrieved documents from Milvus."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "YW-RXYiv-MWp"
      },
      "outputs": [],
      "source": [
        "PROMPT = \"\"\"\n",
        "Use the following pieces of information enclosed in <context> tags to provide an answer to the question enclosed in <question> tags.\n",
        "<context>\n",
        "{context}\n",
        "</context>\n",
        "<question>\n",
        "{question}\n",
        "</question>\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GtRPeLjR-MWp"
      },
      "source": [
        "We use the [Mixtral-8x7B-Instruct-v0.1](https://huggingface.co/mistralai/Mixtral-8x7B-Instruct-v0.1) hosted on Hugging Face inference server to generate a response based on the prompt."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "bYV33WFK-MWp",
        "jupyter": {
          "outputs_hidden": false
        },
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "from huggingface_hub import InferenceClient\n",
        "\n",
        "repo_id = \"mistralai/Mixtral-8x7B-Instruct-v0.1\"\n",
        "\n",
        "llm_client = InferenceClient(model=repo_id, timeout=120)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "tVRwE6Ho-MWp",
        "jupyter": {
          "outputs_hidden": false
        }
      },
      "source": [
        "Finally, we can format the prompt and generate the answer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "8LWdpnMD-MWt",
        "jupyter": {
          "outputs_hidden": false
        },
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "prompt = PROMPT.format(context=context, question=question)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 143
        },
        "id": "hTujHrbkxfXr",
        "outputId": "586f8ee6-8ad5-4c8f-ed16-46957d6e9400"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'\\nUse the following pieces of information enclosed in <context> tags to provide an answer to the question enclosed in <question> tags.\\n<context>\\ntechniques and approaches listed therein.  \\nTITLE II \\nPROHIBITED ARTIFICIAL INTELLIGENCE PRACTICES \\nArticle 5 \\n1. The following artificial intelligence practices shall be prohibited: \\n(a) the placing on the market, putting into service or use of an A I system that \\ndeploys subliminal techniques beyond a person’s consciousness in order to \\nmaterially distort a person’s behaviour in a manner that causes or is likely to \\ncause that person or another person physical or psychological harm; \\n(b) the placing on t he market, putting into service or use of an AI system that \\nexploits any of the vulnerabilities of a specific group of persons due to their \\nage, physical or mental disability, in order to materially distort the behaviour of \\na person pertaining to that grou p in a manner that causes or is likely to cause \\nthat person or another person physical or psychological harm; \\n(c) the placing on the market, putting into service or use of AI systems by public\\nTitle II  establishes a list of prohibited AI. The regulation follows a risk -based approach,  \\ndifferentiating between uses of AI that create (i) an unacceptable risk, (ii) a high risk, and (iii) \\nlow or minimal risk. The list of prohibited practices in Title II comprises all those AI systems \\nwhose use is considered unacceptable as contravening Unio n values, for instance by violating \\nfundamental rights. The prohibitions covers practices that have a significant potential to \\nmanipulate persons  through subliminal techniques beyond their consciousness or exploit\\nEN 13  EN \\nvulnerabilities of specific vulnerable gro ups such as children or persons with disabilities in \\norder to materially distort their behaviour in a manner that is likely to cause them or another \\nperson psychological or physical harm. Other manipulative or exploitative practices affecting \\nadults that m ight be facilitated by AI systems could be covered by the existing data \\nprotection, consumer protection and digital service legislation that guarantee that natural \\npersons are properly informed and have free choice not to be subject to profiling or other \\npractices that might affect their behaviour. The proposal also prohibits AI -based social \\nscoring for general purposes done by public authorities. Finally, the use of ‘real time’ remote \\nbiometric identification systems in publicly accessible spaces for the purpose of law \\nenforcement is also prohibited unless certain limited exceptions apply. \\n5.2.3. HIGH-RISK AI SYSTEMS (TITLE III)\\n</context>\\n<question>\\nWhat are forbidden AI applications?\\n</question>\\n'"
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "prompt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 472
        },
        "id": "huiHo1tU-MWt",
        "jupyter": {
          "outputs_hidden": false
        },
        "outputId": "b144768f-0834-4938-c381-69f4cb0904da",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Forbidden AI applications are those whose use is considered unacceptable as they contravene Union values and violate fundamental rights. These include AI systems that have a significant potential to manipulate persons through subliminal techniques beyond their consciousness or exploit vulnerabilities of specific vulnerable groups such as children or persons with disabilities in order to materially distort their behavior in a manner that is likely to cause them or another person psychological or physical harm. The proposal also prohibits AI-based social scoring for general purposes done by public authorities and the use of 'real time' remote biometric identification systems in publicly accessible spaces for the purpose of law enforcement, except for certain limited exceptions.\n"
          ]
        }
      ],
      "source": [
        "answer = llm_client.text_generation(\n",
        "    prompt,\n",
        "    max_new_tokens=1000,\n",
        ").strip()\n",
        "print(answer)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
