{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning Modelling with Python and Scikit-Learn\n",
    "\n",
    "This notebook explores a variety of essential and practical features within the Scikit-Learn library. \n",
    "\n",
    "Although it covers a lot, it's referred to as a quick overview due to the library's extensive scope. For comprehensive details, it's advisable to consult the full documentation, especially if you encounter any difficulties.\n",
    "\n",
    "You can get more detailed information from the [documentation](https://scikit-learn.org/stable/user_guide.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is Scikit-Learn (sklearn)?\n",
    "\n",
    "[Scikit-Learn](https://scikit-learn.org/stable/index.html), also referred to as `sklearn`, is an open-source Python machine learning library.\n",
    "\n",
    "It's built on top on NumPy (Python library for numerical computing) and Matplotlib (Python library for data visualization).\n",
    "\n",
    "![](images/sklearn-6-step-ml-framework-tools-scikit-learn-highlight.png) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why Scikit-Learn?\n",
    "\n",
    "In data science and machine learning, the primary objective is to identify patterns in data and use them for predictions. \n",
    "\n",
    "Most problems fall into specific categories, such as [classification](https://en.wikipedia.org/wiki/Statistical_classification#Binary_and_multiclass_classification) (e.g., spam vs. non-spam emails), [regression](https://en.wikipedia.org/wiki/Regression_analysis) (e.g., predicting house prices), and [clustering](https://developers.google.com/machine-learning/clustering/overview) (grouping similar samples).\n",
    "\n",
    "Regardless of the problem type, common steps are involved, including dividing data into training and testing sets, selecting a model, and evaluating its performance. Scikit-Learn provides Python tools to handle these tasks efficiently, from data preparation to modeling, thereby saving the effort of building everything from scratch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What does this notebook cover?\n",
    "\n",
    "The Scikit-Learn library is very capable. However, learning everything off by heart isn't necessary. Instead, this notebook focuses some of the main use cases of the library.\n",
    "\n",
    "More specifically, we'll cover:\n",
    "\n",
    "![title](images/sklearn-workflow-title.png)\n",
    "\n",
    "0. Implementing a Complete Scikit-Learn Workflow\n",
    "1. Data Preparation\n",
    "2. Selecting the Appropriate Machine Learning Model\n",
    "3. Training the Model and Making Predictions\n",
    "4. Assessing Model Performance\n",
    "5. Enhancing Predictions via Hyperparameter Tuning\n",
    "6. Saving and Loading Pre-Trained Models\n",
    "7. Integrating Steps into a Unified Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Where can I get help?\n",
    "\n",
    "If you encounter a challenge or think of something not covered in this notebook, don't worry! Here are some recommended steps to help you find a solution:\n",
    "\n",
    "1. **Experiment and Try It Out** - Since Scikit-Learn is designed for ease of use, start by applying what you know and attempting to solve your question on your own. It's okay to make mistakes; they're part of the learning process. If unsure, run your code to see the results.\n",
    "\n",
    "2. **Use Documentation** - Press SHIFT + TAB while inside a function to view its docstring, which provides information about what the function does. Developing this habit will enhance your research skills and deepen your understanding of the library.\n",
    "\n",
    "3. **Search for Solutions** - If experimenting doesn't work, try searching online. You'll likely find answers in one of these places:  \n",
    "* [Scikit-Learn documentation/user guide](https://scikit-learn.org/stable/user_guide.html) : This is the most comprehensive resource for Scikit-Learn information.\n",
    "* [Stack Overflow](https://stackoverflow.com/) : A Q&A platform for developers where you can find solutions to a wide range of software development problems.\n",
    "* [ChatGPT](https://chat.openai.com/) : Useful for explaining code, but be sure to verify any code it generates before using it. Ask it to explain code and follow up with additional questions. Avoid using code you didn't write without verifying it first.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard imports\n",
    "# %matplotlib inline # No longer required in newer versions of Jupyter (2022+)\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import sklearn\n",
    "print(f\"Using Scikit-Learn version: {sklearn.__version__} (materials in this notebook require this version or newer).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Implementing a Complete Scikit-Learn Workflow\n",
    "\n",
    "Before diving into the details, let's take a quick look at what a complete Scikit-Learn workflow looks like. After getting an overview, we'll explore each step more thoroughly.\n",
    "\n",
    "We'll focus on the following hands-on steps:\n",
    "\n",
    "1. **Data Preparation:** Splitting data into features and labels, and setting up training and testing sets.\n",
    "\n",
    "2. **Model Selection:** Choosing the right model for our problem.\n",
    "\n",
    "3. **Model Training and Prediction:** Fitting the model to the data and using it to make predictions.\n",
    "\n",
    "3. **Model Evaluation:** Assessing the model's performance.\n",
    "\n",
    "4. **Improvement through Experimentation:** Enhancing the model's performance.\n",
    "\n",
    "5. **Saving the Model:** Saving a trained model for future use.\n",
    "\n",
    "> **Note:** The upcoming section provides a comprehensive end-to-end workflow, which might be information-dense. We'll cover it quickly and then break it down further throughout the notebook. Keep in mind that Scikit-Learn is a versatile library, and the workflow presented here is just one example of how it can be used.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest Classifier Workflow for Classifying Heart Disease\n",
    "\n",
    "#### 1. Data Preparation\n",
    "\n",
    "As an example dataset, we'll import `heart-disease.csv`.\n",
    "\n",
    "This file contains anonymised patient medical records and whether or not they have heart disease or not (this is a classification problem since we're trying to predict whether something is one thing or another)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "heart_disease = pd.read_csv(\"https://raw.githubusercontent.com/mrdbourke/zero-to-mastery-ml/master/data/heart-disease.csv\") # load data directly from URL (requires raw form on GitHub, source: https://github.com/mrdbourke/zero-to-mastery-ml/blob/master/data/heart-disease.csv)\n",
    "heart_disease.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, each row is a different patient and all columns except `target` are different patient characteristics. \n",
    "\n",
    "The `target` column indicates whether the patient has heart disease (`target=1`) or not (`target=0`), this is our \"label\" columnm, the variable we're going to try and predict.\n",
    "\n",
    "The rest of the columns (often called features) are what we'll be using to predict the `target` value.\n",
    "\n",
    "> **Note:** It's a common custom to save features to a varialbe `X` and labels to a variable `y`. In practice, we'd like to use the `X` (features) to build a predictive algorithm to predict the `y` (labels)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create X (all the feature columns)\n",
    "X = heart_disease.drop(\"target\", axis=1)\n",
    "\n",
    "# Create y (the target column)\n",
    "y = heart_disease[\"target\"]\n",
    "\n",
    "# Check the head of the features DataFrame\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the head and the value counts of the labels \n",
    "y.head(), y.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the most important practices in machine learning is to split datasets into training and test sets.\n",
    "\n",
    "As in, a model will **train on the training set** to learn patterns and then those patterns can be **evaluated on the test set**.\n",
    "\n",
    "Crucially, a model should **never** see testing data during training.\n",
    "\n",
    "Scikit-learn provides the [`sklearn.model_selection.train_test_split`](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html) method to split datasets in training and test sets.\n",
    "\n",
    "> **Note:** A common practice to use an 80/20 or 70/30 or 75/25 split for training/testing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and test sets\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# np.random.seed(42)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, \n",
    "                                                    y,\n",
    "                                                    test_size=0.25) # by default train_test_split uses 25% of the data for the test set\n",
    "\n",
    "X_train.shape, X_test.shape, y_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Model Selection\n",
    "\n",
    "Selecting a model in Scikit-Learn depends on the type of problem you're addressing, such as classification or regression. Scikit-Learn provides a variety of models suitable for each problem type, which can be explored in their machine learning map.\n",
    "\n",
    "In Scikit-Learn, models are referred to as \"estimators,\" though they are commonly known as `model` or `clf` (classifier). Each model has hyperparameters, which are adjustable settings that can be tuned to optimize the model's performance for your specific problem. Hyperparameters can be thought of as knobs on an oven that you adjust to achieve the perfect cooking conditions for your dish."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since we're working on a classification problem, we'll start with a RandomForestClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "clf = RandomForestClassifier()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see the current hyperparameters of a model with the [`get_params()`](https://scikit-learn.org/stable/developers/develop.html#get-params-and-set-params) method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View the current hyperparameters\n",
    "clf.get_params()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll leave this as is for now, as Scikit-Learn models generally have good default settings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Model Training and Prediction\n",
    "\n",
    "Fitting a model a dataset involves passing it the data and asking it to figure out the patterns.\n",
    "\n",
    "If there are labels (supervised learning), the model tries to work out the relationship between the data and the labels. \n",
    "\n",
    "If there are no labels (unsupervised learning), the model tries to find patterns and group similar samples together.\n",
    "\n",
    "Most Scikit-Learn models have the [`fit(X, y)`](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html#sklearn.ensemble.RandomForestClassifier.fit) method built-in, where the `X` parameter is the features and the `y` parameter is the labels.\n",
    "\n",
    "In our case, we start by fitting a model on the training split (`X_train`, `y_train`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.fit(X=X_train, y=y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Use the model to make a prediction\n",
    "\n",
    "The whole point of training a machine learning model is to use it to make some kind of prediction in the future.\n",
    "\n",
    "Once your model instance is trained, you can use the [`predict()`](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html#sklearn.ensemble.RandomForestClassifier.predict) method to predict a target value given a set of features. \n",
    "\n",
    "In other words, use the model, along with some new, unseen and unlabelled data to predict the label.\n",
    "\n",
    "> **Note:** Data you predict on should be in the same shape and format as data you trained on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In order to predict a label, data has to be in the same shape as X_train\n",
    "X_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the model to make a prediction on the test data (further evaluation)\n",
    "y_preds = clf.predict(X=X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_preds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Model Evaluation\n",
    "\n",
    "Now that we've generated predictions, we can use Scikit-Learn methods to assess the quality of our model. Each model, or estimator, includes a built-in [`score()`](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html#sklearn.ensemble.RandomForestClassifier.score) method that evaluates how well the model has learned the relationships between features and labels.\n",
    "\n",
    "The `score()` method utilizes a standard evaluation metric specific to the model type. For classifiers like ours, a common metric is accuracy, which measures the fraction of correct predictions out of the total predictions.\n",
    "\n",
    "Let's examine our model's accuracy on the training set to get an initial sense of its performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model on the training set\n",
    "train_acc = clf.score(X=X_train, y=y_train)\n",
    "print(f\"The model's accuracy on the training dataset is: {train_acc*100}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like our model is doing pretty well. Maybe even suspiciously well. Any ideas what could have happened?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model on the test set\n",
    "test_acc = clf.score(X=X_test, y=y_test)\n",
    "print(f\"The model's accuracy on the testing dataset is: {test_acc*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hmm, looks like our model's accuracy is a bit less on the test dataset than the training dataset.\n",
    "\n",
    "This is quite often the case, because remember, a model has never seen the testing examples before.\n",
    "\n",
    "There are also a number of other evaluation methods we can use for our classification models.\n",
    "\n",
    "All of the following classification metrics come from the [`sklearn.metrics`](https://scikit-learn.org/stable/modules/model_evaluation.html#classification-metrics) module:\n",
    "* [`classification_report(y_true, y_true)`](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.classification_report.html#sklearn.metrics.classification_report) - Builds a text report showing various classification metrics such as [precision, recall](https://scikit-learn.org/stable/auto_examples/model_selection/plot_precision_recall.html) and F1-score.\n",
    "* [`confusion_matrix(y_true, y_pred)`](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.confusion_matrix.html#sklearn.metrics.confusion_matrix) - Create a [confusion matrix](https://en.wikipedia.org/wiki/Confusion_matrix) to compare predictions to truth labels.\n",
    "* [`accuracy_score(y_true, y_pred)`](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.accuracy_score.html#sklearn.metrics.accuracy_score) - Find the accuracy score (the default metric) for a classifier.\n",
    "\n",
    "All metrics have the following in common: they compare a model's predictions (`y_pred`) to truth labels (`y_true`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "\n",
    "# Create a classification report\n",
    "print(classification_report(y_test, y_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a confusion matrix\n",
    "conf_mat = confusion_matrix(y_test, y_preds)\n",
    "conf_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the accuracy score (same as the score() method for classifiers) \n",
    "accuracy_score(y_test, y_preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. Improvement through Experimentation\n",
    "\n",
    "The initial model you create is typically considered a baseline, often simpler than what you've built.\n",
    "\n",
    "To improve upon your baseline, the key strategy is experimentation. Experiments can be categorized into two main types:\n",
    "\n",
    "**Model Perspective:** This involves using more complex models or tuning the hyperparameters of your existing model. For instance, you can adjust the hyperparameters of a RandomForestClassifier() by changing the number of trees (n_estimators) to see how it affects performance.\n",
    "\n",
    "**Data Perspective:** This involves collecting more data or improving the quality of your existing data to help your model learn patterns more effectively.\n",
    "\n",
    "When working with an existing dataset, it's often easier to start with model-based experiments before considering data enhancements if results are unsatisfactory.\n",
    "\n",
    "An important consideration when tuning hyperparameters is to ensure that your results are cross-validated. Cross-validation helps confirm that your model's performance is consistent across different training and test sets, rather than being due to chance. This is crucial for ensuring reliable results.\n",
    "\n",
    "For our [`RandomForestClassifier()`](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html), we can experiment with different values for `n_estimators`, starting with the default of 100 and increasing it to 200 to observe any improvements. Generally, more trees can lead to better performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try different numbers of estimators (trees)... (no cross-validation)\n",
    "np.random.seed(42)\n",
    "for i in range(100, 200, 10):\n",
    "    print(f\"Trying model with {i} estimators...\")\n",
    "    model = RandomForestClassifier(n_estimators=i).fit(X_train, y_train)\n",
    "    print(f\"Model accuracy on test set: {model.score(X_test, y_test) * 100:.2f}%\")\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The metrics above were measured on a single train and test split.\n",
    "\n",
    "Let's use [`sklearn.model_selection.cross_val_score`](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.cross_val_score.html) to measure the results across 5 different train and test sets.\n",
    "\n",
    "We can achieve this by setting `cross_val_score(X, y, cv=5)`.\n",
    "\n",
    "Where `X` is the *full* feature set and `y` is the *full* label set and `cv` is the number of train and test splits `cross_val_score` will automatically create from the data (in our case, `5` different splits, this is known as 5-fold cross-validation)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# With cross-validation\n",
    "np.random.seed(42)\n",
    "for i in range(100, 200, 10):\n",
    "    print(f\"Trying model with {i} estimators...\")\n",
    "    model = RandomForestClassifier(n_estimators=i).fit(X_train, y_train)\n",
    "\n",
    "    # Measure the model score on a single train/test split\n",
    "    model_score = model.score(X_test, y_test)\n",
    "    print(f\"Model accuracy on single test set split: {model_score * 100:.2f}%\")\n",
    "    \n",
    "    # Measure the mean cross-validation score across 5 different train and test splits\n",
    "    cross_val_mean = np.mean(cross_val_score(model, X, y, cv=5))\n",
    "    print(f\"5-fold cross-validation score: {cross_val_mean * 100:.2f}%\")\n",
    "    \n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which model had the best cross-validation score?\n",
    "\n",
    "This is usually a better indicator of a quality model than a single split accuracy score.\n",
    "\n",
    "Rather than set up and track the results of these experiments manually, we can get Scikit-Learn to do the exploration for us.\n",
    "\n",
    "Scikit-Learn's [`sklearn.model_selection.GridSearchCV`](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html) is a way to search over a set of different hyperparameter values and automatically track which perform the best.\n",
    "\n",
    "Let's test it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Another way to do it with GridSearchCV...\n",
    "np.random.seed(42)\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Define the parameters to search over in dictionary form \n",
    "# (these can be any of your target model's hyperparameters) \n",
    "param_grid = {'n_estimators': [i for i in range(100, 200, 10)],\n",
    "              'max_features': [j for j in range(1, 5, 1)]}\n",
    "\n",
    "# Setup the grid search\n",
    "grid = GridSearchCV(estimator=RandomForestClassifier(),\n",
    "                    param_grid=param_grid,\n",
    "                    cv=5,\n",
    "                    verbose=1) \n",
    "\n",
    "# Fit the grid search to the data\n",
    "grid.fit(X, y)\n",
    "\n",
    "# Find the best parameters\n",
    "print(f\"The best parameter values are: {grid.best_params_}\")\n",
    "print(f\"With a score of: {grid.best_score_*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can extract the best model/estimator with the `best_estimator_` attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the model to be the best estimator\n",
    "clf = grid.best_estimator_\n",
    "clf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now we've got the best cross-validated model, we can fit and score it on our original single train/test split of the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the best model\n",
    "clf = clf.fit(X_train, y_train)\n",
    "\n",
    "# Find the best model scores on our single test split\n",
    "# (note: this may be lower than the cross-validation score since it's only on one split of the data)\n",
    "print(f\"Best model score on single split of the data: {clf.score(X_test, y_test)*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6. Save a model for someone else to use\n",
    "\n",
    "When you've done a few experiments and you're happy with how your model is doing, you'll likely want someone else to be able to use it.\n",
    "\n",
    "This may come in the form of a teammate or colleague trying to replicate and validate your results or through a customer using your model as part of a service or application you offer.\n",
    "\n",
    "Saving a model also allows you to reuse it later without having to go through retraining it. Which is helpful, especially when your training times start to increase.\n",
    "\n",
    "You can [save a Scikit-Learn model](https://scikit-learn.org/stable/model_persistence.html) using Python's in-built [`pickle` module](https://docs.python.org/3/library/pickle.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# Save an existing model to file\n",
    "pickle.dump(grid, open(\"random_forest_model_1_grid.pkl\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a saved pickle model and evaluate it\n",
    "loaded_pickle_model = pickle.load(open(\"random_forest_model_1_grid.pkl\", \"rb\"))\n",
    "print(f\"Loaded pickle model prediction score: {loaded_pickle_model.score(X_test, y_test) * 100:.2f}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
